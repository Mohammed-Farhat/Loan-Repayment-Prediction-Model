{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "In this project, we aim to build a predictive model to determine whether a borrower will fully repay a loan or default (charged off), based on historical data provided by LendingClub.\n",
    "\n",
    "This classification task has real-world implications for financial institutions, helping reduce default risk and improve lending decisions.\n",
    "\n",
    "We'll go through a complete machine learning pipeline, including data preprocessing, exploratory analysis, model training, evaluation, and model persistence.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Data Features\n",
    "-----\n",
    "There are many LendingClub data sets on Kaggle. Here is the information on this particular data set:\n",
    "\n",
    "<table border=\"1\" class=\"dataframe\">\n",
    "  <thead>\n",
    "    <tr style=\"text-align: right;\">\n",
    "      <th></th>\n",
    "      <th>LoanStatNew</th>\n",
    "      <th>Description</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <th>0</th>\n",
    "      <td>loan_amnt</td>\n",
    "      <td>The listed amount of the loan applied for by the borrower. If at some point in time, the credit department reduces the loan amount, then it will be reflected in this value.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>1</th>\n",
    "      <td>term</td>\n",
    "      <td>The number of payments on the loan. Values are in months and can be either 36 or 60.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>2</th>\n",
    "      <td>int_rate</td>\n",
    "      <td>Interest Rate on the loan</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>3</th>\n",
    "      <td>installment</td>\n",
    "      <td>The monthly payment owed by the borrower if the loan originates.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>4</th>\n",
    "      <td>grade</td>\n",
    "      <td>LC assigned loan grade</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>5</th>\n",
    "      <td>sub_grade</td>\n",
    "      <td>LC assigned loan subgrade</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>6</th>\n",
    "      <td>emp_title</td>\n",
    "      <td>The job title supplied by the Borrower when applying for the loan.*</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>7</th>\n",
    "      <td>emp_length</td>\n",
    "      <td>Employment length in years. Possible values are between 0 and 10 where 0 means less than one year and 10 means ten or more years.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>8</th>\n",
    "      <td>home_ownership</td>\n",
    "      <td>The home ownership status provided by the borrower during registration or obtained from the credit report. Our values are: RENT, OWN, MORTGAGE, OTHER</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>9</th>\n",
    "      <td>annual_inc</td>\n",
    "      <td>The self-reported annual income provided by the borrower during registration.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>10</th>\n",
    "      <td>verification_status</td>\n",
    "      <td>Indicates if income was verified by LC, not verified, or if the income source was verified</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>11</th>\n",
    "      <td>issue_d</td>\n",
    "      <td>The month which the loan was funded</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>12</th>\n",
    "      <td>loan_status</td>\n",
    "      <td>Current status of the loan</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>13</th>\n",
    "      <td>purpose</td>\n",
    "      <td>A category provided by the borrower for the loan request.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>14</th>\n",
    "      <td>title</td>\n",
    "      <td>The loan title provided by the borrower</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>15</th>\n",
    "      <td>zip_code</td>\n",
    "      <td>The first 3 numbers of the zip code provided by the borrower in the loan application.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>16</th>\n",
    "      <td>addr_state</td>\n",
    "      <td>The state provided by the borrower in the loan application</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>17</th>\n",
    "      <td>dti</td>\n",
    "      <td>A ratio calculated using the borrower’s total monthly debt payments on the total debt obligations, excluding mortgage and the requested LC loan, divided by the borrower’s self-reported monthly income.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>18</th>\n",
    "      <td>earliest_cr_line</td>\n",
    "      <td>The month the borrower's earliest reported credit line was opened</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>19</th>\n",
    "      <td>open_acc</td>\n",
    "      <td>The number of open credit lines in the borrower's credit file.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>20</th>\n",
    "      <td>pub_rec</td>\n",
    "      <td>Number of derogatory public records</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>21</th>\n",
    "      <td>revol_bal</td>\n",
    "      <td>Total credit revolving balance</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>22</th>\n",
    "      <td>revol_util</td>\n",
    "      <td>Revolving line utilization rate, or the amount of credit the borrower is using relative to all available revolving credit.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>23</th>\n",
    "      <td>total_acc</td>\n",
    "      <td>The total number of credit lines currently in the borrower's credit file</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>24</th>\n",
    "      <td>initial_list_status</td>\n",
    "      <td>The initial listing status of the loan. Possible values are – W, F</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>25</th>\n",
    "      <td>application_type</td>\n",
    "      <td>Indicates whether the loan is an individual application or a joint application with two co-borrowers</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>26</th>\n",
    "      <td>mort_acc</td>\n",
    "      <td>Number of mortgage accounts.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>27</th>\n",
    "      <td>pub_rec_bankruptcies</td>\n",
    "      <td>Number of public record bankruptcies</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 1: Data Loading\n",
    "\n",
    "In this section, we load the LendingClub dataset and conduct a quick overview of the data structure.\n",
    "\n",
    "Steps:\n",
    "- Import necessary libraries\n",
    "- Load the dataset into a Pandas DataFrame\n",
    "- Display dataset shape and column information\n",
    "- Preview the first few rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/lending_club_loan_data.csv')\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 2: Exploratory Data Analysis (EDA)\n",
    "\n",
    "EDA helps uncover trends, patterns, and relationships in the data.\n",
    "\n",
    "In this section, we:\n",
    "- Visualize class imbalance between loan statuses\n",
    "- Explore distributions of numerical and categorical features\n",
    "- Analyze correlations with the target variable (`loan_status`)\n",
    "- Identify potential features for modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**create a countplot of the (`loan_status`) column.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x='loan_status',data=df,hue='loan_status',legend=False)\n",
    "plt.savefig(\"plots/CountPlot_of_LoanStatus.png\", dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create a histogram of the (`loan_amnt`) column.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,6))  \n",
    "sns.histplot(df['loan_amnt'], bins=40, edgecolor=None, alpha=0.7)\n",
    "plt.savefig(\"plots/Hits_Count_Histplot_of_LoanAmount.png\", dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Correlation between all continuous numeric variables using .corr() method.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.corr(numeric_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Visualize using a heatmap**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_df=df.corr(numeric_only=True)\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(corr_df, annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5)\n",
    "plt.savefig(\"plots/Heatmap_of_Corr.png\", dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We have noticed almost perfect correlation with the (`installment`) feature. So we must explore this feature more**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['loan_amnt'].describe())\n",
    "print(\"\\n\")\n",
    "print(df['installment'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "sns.scatterplot(x='installment', y='loan_amnt', data=df)\n",
    "plt.savefig(\"plots/Scatterplot_of_Installment_vs_LoanAmount.png\", dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create a boxplot showing the relationship between the (`loan_status`) and the (`loan_amnt`).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x='loan_status', y='loan_amnt', data=df, palette='Set1')\n",
    "plt.savefig(\"plots/Boxplot_of_LoanStatus_vs_Loanamount.png\", dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Calculate the summary statistics for the loan amount, grouped by the loan_status.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['loan_amnt'].groupby(df['loan_status']).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now let's explore the Grade and SubGrade columns that LendingClub attributes to the loans.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grades=df['grade'].to_numpy()\n",
    "grades = np.array(sorted(set(grades)))  \n",
    "grades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_grades=df['sub_grade'].to_numpy()\n",
    "sub_grades = np.array(sorted(set(sub_grades)))  \n",
    "sub_grades"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create a countplot per (`grade`) column.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x='grade', data=df,hue='loan_status', palette='Set1')\n",
    "plt.savefig(\"plots/Countplot_of_Grade.png\", dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create a count plot per (`sub_grade`)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,6))\n",
    "sns.countplot(x='sub_grade', data=df,palette='coolwarm',order=sub_grades)\n",
    "plt.savefig(\"plots/Countplot_of_SubGrade.png\", dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create a count plot per (`sub_grade`), but here we will use hue as (`loan_status`) to compare**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,6))\n",
    "sns.countplot(x='sub_grade', data=df,palette='coolwarm',order=sub_grades, hue='loan_status')\n",
    "plt.savefig(\"plots/Countplot_of_SubGrade_w_hue_LoanStatus.png\", dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 3: Feature Engineering and Data PreProcessing\n",
    "\n",
    "Here we transform the data into a format suitable for machine learning algorithms.\n",
    "\n",
    "Steps include:\n",
    "- Encoding categorical variables (e.g., one-hot encoding)\n",
    "- Scaling or normalizing numerical features\n",
    "- Creating new features (e.g., interaction terms or flag variables)\n",
    "- Removing highly correlated or irrelevant features\n",
    "- Identify and handle missing values\n",
    "- Drop irrelevant or redundant columns\n",
    "- Ensure correct data types\n",
    "- Remove duplicate rows (if any)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now, We will create a new column called (`loan_repaid`) which will contain a 1 if the (`loan_status`) was \"Fully Paid\" and a 0 if it was \"Charged Off\".**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "status=df['loan_status'].to_numpy()\n",
    "status = np.array(sorted(set(status)))  \n",
    "status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ispaid(x):\n",
    "    if x==\"Fully Paid\":\n",
    "        return 1\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['loan_repaid'] = df['loan_status'].apply(ispaid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['loan_status', 'loan_repaid']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create a bar plot showing the correlation of the numeric features to the new (`loan_repaid`) column.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.corr(numeric_only=True)['loan_repaid'].drop('loan_repaid').dropna().sort_values().plot(kind='bar')\n",
    "plt.savefig(\"plots/Barplot_of_Corr_with_LoanRepaid.png\", dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Our new Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing Data\n",
    "\n",
    "**Let's explore this missing data columns. We use a variety of factors to decide whether or not they would be useful, to see if we should keep, discard, or fill in the missing data.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create a Series that displays the total count of missing values per column.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_count= df.isnull().sum()\n",
    "total_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We check missing data based on the total count of data.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_count = (total_count / len(df)) * 100\n",
    "total_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***We will cover the missing data in each feature by itself and do the best approuch we can inorder to get better outputs***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We check how many unique employment job titles are there**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['emp_title'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['emp_title'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**There are no options to do to save this feature, we cannot convert it to dummy variables due to the large number of different values, so the bewst solution is to drop it**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=['emp_title'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create a count plot of the (`emp_length`) feature column.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['emp_length'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order = ['< 1 year', '1 year', '2 years', '3 years','4 years','5 years','6 years','7 years','8 years', '9 years','10+ years']\n",
    "order\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,5))\n",
    "sns.countplot(x='emp_length', data=df, order=order, palette='Set1')\n",
    "plt.savefig(\"plots/Countplot_of_EmpLength.png\", dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plot out the countplot with a hue separating Fully Paid vs Charged Off**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,5))\n",
    "sns.countplot(x='emp_length', data=df, order=order,hue='loan_status', palette='Set1')\n",
    "plt.savefig(\"plots/Countplot_of_EmpLength_w_hue_LoanStatus.png\", dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This still doesn't really inform us if there is a strong relationship between employment length and being charged off, what we want is the percentage of charge offs per category. Essentially informing us what percent of people per employment category didn't pay back their loan. There are a multitude of ways to create this Series.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emp_co = df[df['loan_status']==\"Charged Off\"]['emp_length'].value_counts()\n",
    "emp_co"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emp_fp = df[df['loan_status']==\"Fully Paid\"]['emp_length'].value_counts()\n",
    "emp_fp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "per_cat=emp_co/emp_fp\n",
    "per_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,4))\n",
    "sns.barplot(per_cat, palette='Set1')\n",
    "plt.savefig(\"plots/Countplot_of_Emplength_w_per_cat.png\", dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Charge off rates are extremely similar across all employment lengths, so we will drop the (`emp_length`)column.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=['emp_length'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Revisit the DataFrame to see what feature columns still have missing data.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Review the title column vs the purpose column.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['purpose'].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['purpose'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['title'].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['title'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The title column is simply a string subcategory/description of the purpose column. So, we will drop the title column.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=['title'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create a (`value_counts`) of the (`mort_acc`) column.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value_counts=df['mort_acc'].value_counts()\n",
    "value_counts    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's review the other columsn to see which most highly correlates to (`mort_acc`)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.corr(numeric_only=True)['mort_acc']\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Looks like the (`total_acc`) feature correlates with the (`mort_acc`) , this makes sense! Let's try this fillna() approach. We will group the dataframe by the (`total_acc`) and calculate the mean value for the (`mort_acc`) per (`total_acc`) entry.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mort_acc_mean = df.groupby('total_acc')['mort_acc'].mean()\n",
    "mort_acc_mean\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's fill in the missing (`mort_acc`) values based on their total_acc value. If the mort_acc is missing, then we will fill in that missing value with the mean value corresponding to its (`total_acc`) value from the Series we created above. This involves using an .apply() method with two columns.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_missing(tot,mort):\n",
    "    if pd.isnull(mort):\n",
    "        return mort_acc_mean[tot]\n",
    "    return mort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['mort_acc'] = df.apply(lambda row: fill_missing(row['total_acc'],row['mort_acc']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['mort_acc'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We will recheck the left features that have missing data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`revol_util` and the `pub_rec_bankruptcies` have missing data points, but they account for less than 0.5% of the total data. We will remove the rows that are missing those values in those columns with dropna().**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical Variables and Dummy Variables\n",
    "\n",
    "**We're done working with the missing data! Now we just need to deal with the string values due to the categorical columns.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**List all the columns that are currently non-numeric**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select_dtypes(include=object).columns    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Let's now go through all the string features to see what we should do with them.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### `term` feature\n",
    "\n",
    "**Convert the term feature into either a 36 or 60 integer numeric data type.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['term']=df['term'].apply(lambda x: int(x.split()[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['term'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `grade` feature\n",
    "\n",
    "**We already know grade is part of sub_grade, so just drop the grade feature.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns='grade', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `sub_grade` feature\n",
    "\n",
    "**We will convert the subgrade into dummy variables. Then concatenate these new columns to the original dataframe.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummies = pd.get_dummies(df['sub_grade'], drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.concat([df.drop(columns=['sub_grade']), dummies], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select_dtypes(include=object).columns    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `verification_status`, `application_type`, `initial_list_status`, and `purpose` features\n",
    "**We will convert these columns into dummy variables and concatenate them with the original dataframe.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vs_dummies=pd.get_dummies(df['verification_status'], drop_first=True)\n",
    "df=pd.concat([df.drop(columns=['verification_status']), vs_dummies], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "at_dummies=pd.get_dummies(df['application_type'], drop_first=True)\n",
    "df=pd.concat([df.drop(columns=['application_type']), at_dummies], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ils_dummies=pd.get_dummies(df['initial_list_status'], drop_first=True)\n",
    "df=pd.concat([df.drop(columns=['initial_list_status']), ils_dummies], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_dummies=pd.get_dummies(df['purpose'], drop_first=True)\n",
    "df=pd.concat([df.drop(columns=['purpose']), p_dummies], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `home_ownership` feature\n",
    "**We will review its value_counts.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['home_ownership'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We will convert these to dummy variables, but replace NONE and ANY with OTHER, so that we end up with just 4 categories, MORTGAGE, RENT, OWN, OTHER. Then concatenate them with the original dataframe.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['home_ownership'] = df['home_ownership'].replace(['NONE', 'ANY'], 'OTHER')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['home_ownership'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ho_dummies=pd.get_dummies(df['home_ownership'], drop_first=True)\n",
    "df=pd.concat([df.drop(columns=['home_ownership']), ho_dummies], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `address` feature\n",
    "**Now let's feature engineer a zip code column from the address in the data set.We will Create a column called `zip_code` that extracts the zip code from the address column.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['zip_code']=df['address'].apply(lambda x: x.split()[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['zip_code'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now we will  make this `zip_code` column into dummy variables,concatenate the result, and drop the original `zip_code` column along with dropping the `address` column.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zc_dummies=pd.get_dummies(df['zip_code'],drop_first=True)\n",
    "df=pd.concat([df.drop(['zip_code','address'], axis=1),zc_dummies], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `issue_d` feature\n",
    "\n",
    "**This would be data leakage, we wouldn't know beforehand whether or not a loan would be issued when using our model, so in theory we wouldn't have an `issue_d`, So we drop this feature.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['issue_d'],inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `earliest_cr_line` feature\n",
    "**This appears to be a historical time stamp feature. We will extract the year from this feature using a .apply function, then convert it to a numeric feature. Then set this new data to a feature column called `earliest_cr_year`, then drop the `earliest_cr_line` feature.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['earliest_cr_year']=df['earliest_cr_line'].apply(lambda x: int(x.split('-')[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['earliest_cr_line'],inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 4: Train Test Split\n",
    "\n",
    "To evaluate model generalization, we split the dataset into:\n",
    "- **Training set**: used to train the model\n",
    "- **Testing set**: used to evaluate model performance on unseen data\n",
    "\n",
    "We typically use an 80/20 or 70/30 split."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the data\n",
    "\n",
    "**We will split the data into training and testing splits.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['loan_status'], inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X= df.drop('loan_repaid', axis=1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y= df['loan_repaid'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=101)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalizing the Data\n",
    "\n",
    "**We will use a MinMaxScaler to normalize the feature data X_train and X_test.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler=MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = scaler.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test=scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 5: Model Building\n",
    "\n",
    "In this section, we build machine learning models to classify loan status.\n",
    "\n",
    "Models used:\n",
    "- **Artificial Neural Network (ANN)** using Keras\n",
    "- **Random Forest Classifier** using scikit-learn\n",
    "\n",
    "We fit each model on the training data and compare their capabilities.\n",
    "\n",
    "Hyperparameters and architectures are chosen based on experimentation or grid search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense,Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ANN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(128,  activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(units=1,activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit( x=X_train,y=y_train,epochs=25,batch_size=256,validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss=pd.DataFrame(model.history.history)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.plot(figsize=(12,6))\n",
    "plt.savefig(\"plots/Loss_vs_ValLoss_of_ANN.png\", dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf=RandomForestClassifier(class_weight='balanced',random_state=101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_depth': [None, 10, 20],\n",
    "    'min_samples_split': [2, 5],\n",
    "    'min_samples_leaf': [1, 2]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search_rf = GridSearchCV(estimator=rf, param_grid=param_grid, \n",
    "                              scoring='f1', cv=3, verbose=2, n_jobs=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search_rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 6: Model Evaluatione.\n",
    "\n",
    "We evaluate model performance using several metrics:\n",
    "\n",
    "- **Confusion Matrix**: true positives, false positives, etc.\n",
    "- **Accuracy**: overall correctness\n",
    "- **Precision & Recall**: especially important with imbalanced data\n",
    "- **F1 Score**: harmonic mean of precision and recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report,confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**For Ann Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_pred=model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test,ann_pred.round()))\n",
    "print(\"\\n\")\n",
    "print(confusion_matrix(y_test,ann_pred.round()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**For Random Forest Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_pred=grid_search_rf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test,rf_pred.round()))\n",
    "print(\"\\n\")\n",
    "print(confusion_matrix(y_test,rf_pred.round()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Secrtion 7: Training/Testing with a Random Row\n",
    "\n",
    "After training and evaluating our models on the test set, it's valuable to simulate how the model would behave in a real-world scenario.\n",
    "\n",
    "In this section, we:\n",
    "- Randomly select a sample from the original dataset (excluding the target).\n",
    "- Feed the row into the trained model to get predictions.\n",
    "- Compare the predicted labels with the actual `loan_status` values (if available).\n",
    "\n",
    "This process helps validate that the model can generalize to individual, unseen cases — not just overall metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fully_paid_df=df[df['loan_repaid']==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(101)\n",
    "random_ind_1 = random.randint(0,len(fully_paid_df))\n",
    "\n",
    "new_customer_1 = fully_paid_df.drop('loan_repaid',axis=1).iloc[random_ind_1]\n",
    "new_customer_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(new_customer_1.astype(float).values.reshape(1, -1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now lets check whether the perosn paid his loan or no**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fully_paid_df.iloc[random_ind_1]['loan_repaid']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Both are 1, so the model is correct in predicting that this customer will repay the loan."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "charged_off_df=df[df['loan_repaid']==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed()\n",
    "random_ind_0 = random.randint(0,len(charged_off_df))\n",
    "\n",
    "new_customer_0 = charged_off_df.drop('loan_repaid',axis=1).iloc[random_ind_0]\n",
    "new_customer_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(new_customer_0.astype(float).values.reshape(1, -1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now lets check whether the perosn paid his loan or no**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "charged_off_df.iloc[random_ind_0]['loan_repaid']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Oops here we have a problem with the inbalance class 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Secrtion 8: Saving Models\n",
    "\n",
    "After training and validating the models, we save them for future inference.\n",
    "\n",
    "Tools used:\n",
    "- `joblib` for scikit-learn models\n",
    "- `.h5` format for Keras models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(grid_search_rf, 'models/rf_model.pkl')\n",
    "model.save('models/ann_model.h5')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# THANK YOU!!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
